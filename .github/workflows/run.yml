name: Esegui Scraper Probabili Formazioni

# 1. Trigger del Workflow
on:
  # Usa 'workflow_dispatch' per permettere l'esecuzione manuale dalla scheda Actions
  workflow_dispatch:

jobs:
  run_scraper:
    # Esegue il job sul runner Linux (ubuntu-latest)
    runs-on: ubuntu-latest
    # Imposta un timeout massimo di 15 minuti per il job (utile per lo scraping)
    timeout-minutes: 15 

    steps:
    # Step 1: Clona il tuo repository sul runner
    - uses: actions/checkout@v4

    # Step 2: Configura l'ambiente Python
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    # Step 3: Installa le dipendenze Python e i browser di Playwright
    - name: Install dependencies and Playwright Browsers
      run: |
        # Installazione normale
        pip install -r requirements.txt
        # Installazione esplicita della libreria che gestisce il client Drive (sicurezza)
        pip install google-api-python-client
        # Installa il browser Chromium necessario per lo scraping
        playwright install chromium
        
    # Step 4: Esecuzione dello script Python consolidato (e autenticazione)
    - name: Run Python Scraper and Upload to Drive
      # Passiamo il secret GOOGLE_CREDENTIALS_B64 allo script Python come variabile d'ambiente
      env:
        GOOGLE_CREDENTIALS_B64: ${{ secrets.GOOGLE_CREDENTIALS_B64 }}
      run: python run.py
      
    # Step 5 (Opzionale ma Utile per Debug): Carica gli screenshot come Artifacts
    - name: Upload Screenshots Artifact
      uses: actions/upload-artifact@v4
      with:
        # Nome dell'archivio zip da scaricare
        name: screenshots-risultati
        # Percorsi dei file da includere nell'archivio
        path: |
          sosfanta_*.png
          fantacalcio_*.png
          gazzetta_*.png
        # Consente la sovrascrittura nelle successive esecuzioni
        overwrite: true
